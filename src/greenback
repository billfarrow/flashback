#!/usr/bin/python -B

"""
TODO LIST

NOW
~~~
[?] snazzy name
[x] config files (global, per machine, per share)
[x] use rsync instead of rsback
[x] rotate backups myself
[x] plug-in to allow LED blinking
[x] store status in a file
[x] debian package
[ ] push to github
[ ] write summary info to a per-backup status/history file (sqlite?)
[x] second-level backups
[ ] bug?? : failures (half-copies) still update daily.1 timestamp
[ ] install monitor.sh in examples
[ ] test start/stop/install/uninstall
[x] top-level config file - change from import to config format
[x] top-level config file - move to /etc
[x] config file - allow comments
[ ] config file - user-defined "topdir"
[ ] config file - default cycle times, labels
[ ] rsyncOpts does not work

LATER
~~~~~
audit runs after backup, compares "new" and "large" files to yesterday's large files, compares md5, creates hard links
"""

import os
import datetime
import subprocess
import shlex
import glob
import operator
import sys
from optparse import OptionParser
import time
import shutil
# from configobj import ConfigObj         # apt-get install python-configobj
import imp
import errno


#-----------------------------------------------------------
#  C L A S S E S
#-----------------------------------------------------------

class DefaultsClass:
    def __init__ (self, *argv, **argd):
        self.__dict__.update (argd)

#-----------------------------------------------------------

def enum(*sequential, **named):
    enums = dict(zip(sequential, range(len(sequential))), **named)
    return type('Enum', (), enums)

#-----------------------------------------------------------
#  S T A R T
#-----------------------------------------------------------

# globals
programName = 'greenback'
topdir = '/backup'
libdir = '/var/lib/'+programName
maxAge = 10000000
sleepMin = 10
# defaults (globals)
class defaults (DefaultsClass):
    lastBackupTimestamp = datetime.datetime(1970,1,1)
    cycleSec = 24*60*60
    keepCount = 9
    label = 'daily'
# plumbing (globals)
options = ()
g_logFD = None
status = enum('UNKNOWN', 'NOT_READY', 'READY', 'SUCCEEDED', 'FAILED', 'DISABLED')

#-----------------------------------------------------------

def jobfile_to_array(filename):
    try:
        log_debug("config file '"+filename+"' found")
        configFilePtr = open( filename, "r" )
    except OSError:
        log_debug("config file '"+filename+"' not found")
        pass

    # MAKE BASIC ENTRIES IN THE 'jobInfo' DICTIONARY FROM /etc/greenback.jobs

    sanitizedJobs = []
    for line in configFilePtr:
        line = line.partition('#')[0]  # strip comments
        line = line.rstrip('\r\n')     # strip trailing CR/LF
        line = line.strip(' ')         # strip leading space
        log_debug(filename+": "+line)
        pieces = line.split(' ')
        rawJob = {}   # raw junk read from config file
        for piece in pieces:
            keyValuePair = piece.split('=')
            if len(keyValuePair) == 2:
                key = keyValuePair[0]
                value = keyValuePair[1]
                rawJob[key] = value
        if len(rawJob) == 0:
            continue
        log_debug('raw job: '+','.join(['%s>%s' % (key, value) for (key, value) in rawJob.items()]) )

        sanitizedJob = {}   # sanitized job description
        # STRING OPTIONS
        req1 = setParmString(filename,rawJob,'host',sanitizedJob,'host')
        req2 = setParmString(filename,rawJob,'volume',sanitizedJob,'volume')
        req3 = setParmString(filename,rawJob,'src',sanitizedJob,'src')
        setParmString(filename,rawJob,'label',sanitizedJob,'label')
        setParmString(filename,rawJob,'rsyncOpts',sanitizedJob,'rsyncOpts')
        setParmString(filename,rawJob,'tool',sanitizedJob,'tool')
        # NUMBER OPTIONS
        setParmInt(filename,rawJob,'cycleDay',sanitizedJob,'cycleSec',86400)
        setParmInt(filename,rawJob,'cycleHour',sanitizedJob,'cycleSec',3600)
        setParmInt(filename,rawJob,'cycleMin',sanitizedJob,'cycleSec',60)
        setParmInt(filename,rawJob,'cycleSec',sanitizedJob,'cycleSec')
        setParmInt(filename,rawJob,'keepCount',sanitizedJob,'keepCount')
        # BOOLEAN OPTIONS
        setParmBool(filename,rawJob,'disabled',sanitizedJob,'disabled')

        # IF THE JOB INFO IS COMPLETE, THEN SAVE IT
        if len(sanitizedJob) > 0 :
            if req1 and req2 and req3:
                log_debug('sanitizedJob:'+','.join(['%s>%s' % (key, value) for (key, value) in sanitizedJob.items()]) )
                sanitizedJobs.append(sanitizedJob)
            else:
                log_error('incomplete job '+','.join(['%s>%s' % (key, value) for (key, value) in sanitizedJob.items()]) )

    log_debug('end of '+filename)
    return sanitizedJobs

#-----------------------------------------------------------

def cfgfile_to_dict(filename):
    try:
        log_debug("config file '"+filename+"' found")
        handle = open( filename, "r" )
    except OSError:
        log_debug("config file '"+filename+"' not found")
        return dict()

    # PARSE CONFIG FILE, BUILD DICTIONARY

    raw = {}
    for line in handle:
        line = line.partition('#')[0]  # strip comments
        line = line.rstrip('\r\n')     # strip trailing CR/LF
        line = line.strip(' ')         # strip leading space
        if len(line) == 0:
            continue
        if line.find('=') == -1:
            log_error('unknown config file option in '+filename+' > '+line)
            continue
        log_debug(filename+": "+line)
        (key,eq,value) = line.partition('=')
        key = key.strip()
        value = value.strip()
        raw[key] = value
        log_debug(filename+" "+key+":"+value)

    handle.close()
    log_debug('raw '+','.join(['%s:%s' % (key, value) for (key, value) in raw.items()]) )
    return raw

#-----------------------------------------------------------

def log_init():
    # we're writing these globals
    global g_logFD
    # log file
    #  logdir = os.environ['HOME']+"/var/log"
    #  if not os.path.exists(logdir):
    #     os.makedirs(logdir)
    #  logfile = logdir+"/garage.log"
    #  logfile = programName+'.log'
    logfile = '/dev/stdout'
    g_logFD = open(logfile,'a')

#-----------------------------------------------------------

def log_debug(string):
    if options.debug: log_info(string)

#-----------------------------------------------------------

def log_info(string):
    if options.quiet: return
    global g_logFD
    timeStamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    g_logFD.write(timeStamp+" "+string+"\n")
    g_logFD.flush()
    ##os.fsync(g_logFD)

#-----------------------------------------------------------

def log_error(string):
    global g_logFD
    timeStamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    g_logFD.write(timeStamp+" ERROR "+string+"\n")
    g_logFD.flush()
    ##os.fsync(g_logFD)

#-----------------------------------------------------------

def shell_capture(cmdargs):
    global g_logFD
    log_debug('shell_capture command >> '+(' '.join(cmdargs)))
    p = subprocess.Popen(cmdargs, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = p.communicate()
    rc = p.returncode
    log_debug("shell_capture done, rc="+('%d'%rc))
    return rc, stdout, stderr

#-----------------------------------------------------------

def shell_do(cmdargs):
    global g_logFD
    log_debug('shell_do command >> '+(' '.join(cmdargs)))
    rc = subprocess.call(cmdargs)
    #rc = subprocess.call(cmdargs, stdout=g_logFD, stderr=g_logFD)
    log_debug("shell_do rc = "+("%d"%rc))
    return rc

#-----------------------------------------------------------

def sec2dhms(s):
    days = s // 86400  ; s = s - (days * 86400)
    hours = s // 3600  ; s = s - (hours * 3600)
    mins = s // 60     ; s = s - (mins * 60)
    secs = s
    return (days, hours, mins, secs)

#-----------------------------------------------------------

def mkdir_p(path):
    try:
        os.makedirs(path)
    except OSError as exc: # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else: raise

#-----------------------------------------------------------

def write_status(doing,waitTime,target):
    log_debug('writing status file')
    statusFileName = libdir+'/status'
    statusFile = file(statusFileName,'w')
    now = datetime.datetime.now()
    statusFile.write('date='+datetime.datetime.strftime(now,'%Y-%m-%d')+'\n')
    statusFile.write('time='+datetime.datetime.strftime(now,'%H:%M:%S')+'\n')
    statusFile.write('pid='+str(os.getpid())+'\n')
    statusFile.write('wait='+str(waitTime)+'\n')
    statusFile.write('status='+doing+'\n')
    if target is None: target=''
    statusFile.write('target='+target+'\n')
    (total, used, free) = disk_usage(topdir)
    statusFile.write('disk.mntpt='+topdir+'\n')
    statusFile.write('disk.total='+str(total)+'\n')
    statusFile.write('disk.used='+str(used)+'\n')
    statusFile.write('disk.free='+str(free)+'\n')
    statusFile.close()

#-----------------------------------------------------------

def disk_usage(path):
    """Return disk usage statistics about the given path.

    Returned value is a tuple with three attributes:
    'total', 'used' and 'free' (in bytes).
    """
    st = os.statvfs(path)
    free = st.f_bavail * st.f_frsize
    total = st.f_blocks * st.f_frsize
    used = (st.f_blocks - st.f_bfree) * st.f_frsize
    return (total, used, free)

#-----------------------------------------------------------

def setParmString(cfgFile,cfgHash,cfgIdx,volHash,volIdx):
    if cfgIdx not in cfgHash: return False
    volHash[volIdx] = cfgHash[cfgIdx]
    log_info(' - '+cfgFile+': '+cfgIdx+'='+cfgHash[cfgIdx]+' >> STRING >> '+volIdx+'='+volHash[volIdx])
    del cfgHash[cfgIdx]
    return True

#-----------------------------------------------------------

def setParmInt(cfgFile,cfgHash,cfgIdx,volHash,volIdx,multiplier=1):
    if cfgIdx not in cfgHash: return False
    volHash[volIdx] = int(cfgHash[cfgIdx]) * multiplier
    log_info(' - '+cfgFile+': '+cfgIdx+'='+cfgHash[cfgIdx]+' >> INT >> '+volIdx+'='+str(volHash[volIdx]))
    del cfgHash[cfgIdx]
    return True

#-----------------------------------------------------------

def setParmBool(cfgFile,cfgHash,cfgIdx,volHash,volIdx):
    if cfgIdx not in cfgHash: return False
    if cfgHash[cfgIdx].lower() in ['yes','y','true','1']:
        volHash[volIdx] = True
    else:
        volHash[volIdx] = False
    log_info(' - '+cfgFile+': '+cfgIdx+'='+cfgHash[cfgIdx]+' >> BOOL >> '+volIdx+'='+( 'TRUE' if volHash[volIdx] else 'FALSE'))
    del cfgHash[cfgIdx]
    return True

#-----------------------------------------------------------

def buildJobTable():

    jobInfo = jobfile_to_array('/etc/greenback.jobs')

    # FILL IN THE 'jobInfo' DICTIONARY FROM DEFAULTS AND MORE CONFIGS

    for job in jobInfo:

        log_debug('job:'+','.join(['%s>%s' % (key, value) for (key, value) in job.items()]) )

        # set some defaults - always
        job['status'] = status.UNKNOWN
        job['lastBackupDurationSec'] = 0
        # set some defaults - only if not set in main jobs config file (TODO: change this ???)
        if 'label' not in job: job['label'] = defaults.label
        if 'cycleSec' not in job: job['cycleSec'] = defaults.cycleSec
        if 'keepCount' not in job: job['keepCount'] = defaults.keepCount
        if 'disabled' not in job: job['disabled'] = False
        if 'tool' not in job: job['tool'] = 'rsync'
        if 'rsyncOpts' not in job: job['rsyncOpts'] = ''

        # set an index to refer to this entry by
        index = job['host']+'-'+job['volume']+'-'+job['label']
        job['index'] = index

        # build a list of several optional config files to look for
        configFilenames=[]
        configFilenames.append(topdir+'/config') # /backup/config
        configFilenames.append(topdir+'/'+job['host']+'/config') # /backup/host/config
        configFilenames.append(topdir+'/'+job['host']+'/'+job['volume']+'/config') # /backup/host/volume/config

        for configFilename in configFilenames:
            # read the config files if they exist
            if os.path.isfile(configFilename):
                log_debug("reading config file '"+configFilename+"'")
                rawconfig = cfgfile_to_dict(configFilename)
                # STRING OPTIONS
                setParmString(configFilename,rawconfig,'src',job,'src')
                setParmString(configFilename,rawconfig,'label',job,'label')
                setParmString(configFilename,rawconfig,'rsyncOpts',job,'rsyncOpts')
                setParmString(configFilename,rawconfig,'tool',job,'tool')
                # NUMBER OPTIONS
                setParmInt(configFilename,rawconfig,'cycleDay',job,'cycleSec',86400)
                setParmInt(configFilename,rawconfig,'cycleHour',job,'cycleSec',3600)
                setParmInt(configFilename,rawconfig,'cycleMin',job,'cycleSec',60)
                setParmInt(configFilename,rawconfig,'cycleSec',job,'cycleSec')
                setParmInt(configFilename,rawconfig,'keepCount',job,'keepCount')
                # BOOLEAN OPTIONS
                setParmBool(configFilename,rawconfig,'disabled',job,'disabled')

                if len(rawconfig) > 0:
                    for key in rawconfig.keys():
                        log_error("unknown parameter '"+key+"' in '"+configFilename+"'")
                    sys.exit(1)

            else:
                log_debug("no config file '"+configFilename+"'")

        # Get the creation time of the daily.1 directory, if it exists.
        recentBackup = topdir+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.1'
        # Note: ctime() does not refer to creation time on *nix systems,
        # but rather the last time the inode data changed.
        if os.path.exists(recentBackup):
            mtime = os.path.getmtime(recentBackup)
            job['lastBackupTimestamp'] = datetime.datetime.fromtimestamp(mtime)
        else:
            job['lastBackupTimestamp'] = defaults.lastBackupTimestamp

    return jobInfo

#-----------------------------------------------------------

def updateAgesAndSort(jobInfo):

    # GO THROUGH THE LIST IN ORDER, DETERMINE THEIR AGES AND NEXT BACKUP TIME

    now = datetime.datetime.now()
    for job in jobInfo:
        ageDelta = now - job['lastBackupTimestamp']
        job['ageSec'] = ageDelta.seconds + (ageDelta.days * 86400)
        job['nextBackupSec'] = job['cycleSec'] - job['ageSec']
        log_debug('index='+job['index']+', nextBackupSec='+str(job['nextBackupSec'])+'sec')
        # force disabled backups to the bottom of the list
        if job['disabled']: job['nextBackupSec'] = maxAge

    for job in jobInfo:
        if job['status'] in (status.NOT_READY, status.UNKNOWN):
            # If "cycleSec" has transpired since our last backup, we're "ready".
            if job['ageSec'] > job['cycleSec']: job['status'] = status.READY
            else: job['status'] = status.NOT_READY
        # No matter if "ready" or not, if disabled, don't back up.
        if job['disabled']: job['status'] = status.DISABLED

    sortedJobs = sorted(jobInfo, key=operator.itemgetter('nextBackupSec'))

    return sortedJobs

#-----------------------------------------------------------

def formattedTable(jobs):
    widths = { 'index':0, 'lastBackupTimestamp':0, 'ageSec':0, 'cycleSec':0 }
    for job in jobs:
        for fld in ('index','ageSec','cycleSec'):
            widths[fld] = max(widths[fld], len(str(job[fld])))
    #widths['lastBackupTimestamp'] = len('| 2013-03-20 21:05:18 | 1368/1440  |  NEXT RUN 0d+0:01:12')
    widths['lastBackupTimestamp'] = len('2013-03-20 21:05:18')

    table = []
    table.append(''
        + 'INDEX'.center(widths['index'])                     + '   '
        + 'LAST BACKUP'.center(widths['lastBackupTimestamp']) + '   '
        + 'AGE'.center(widths['ageSec'])                      + '/'
        + 'CYCLE'.center(widths['cycleSec'])                  + '   '
        + 'STATUS')

    now = datetime.datetime.now()
    for job in jobs:
        (d,h,m,s) = sec2dhms( job['nextBackupSec'] )
        nextBackupInterval = '%dd+%d:%02d:%02d' % (d,h,m,s)
        (d,h,m,s) = sec2dhms( job['lastBackupDurationSec'] )  # assume 0 days
        lastBackupDurationSec = '%d:%02d:%02d' % (h,m,s)
        switch = {
            status.UNKNOWN:   '???',
            status.DISABLED:  'DISABLED',
            status.NOT_READY: 'NEXT RUN IN '+nextBackupInterval,
            status.READY:     'READY',
            status.SUCCEEDED: 'SUCCEEDED ('+lastBackupDurationSec+')',
            status.FAILED:    'FAILED ('+lastBackupDurationSec+')',
        }
        lastBackupTimestampString = job['lastBackupTimestamp'].strftime('%Y-%m-%d %H:%M:%S')
        table.append(''
            + job['index'].ljust(widths['index'])            + '   '
            + lastBackupTimestampString                         + '   '
            + str(job['ageSec']).rjust(widths['ageSec'])     + '/'
            + str(job['cycleSec']).ljust(widths['cycleSec']) + '   '
            + switch[job['status']]
        )

    return table

#-----------------------------------------------------------

def tableToLogAndDisk(activity,jobInfo):

    # CREATE A FORMATTED TABLE

    table = formattedTable(jobInfo)
    date = datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S')

    # DUMP THE SORTED LIST IN A TEXT FILE

    tableFileName = libdir+'/queue'
    tableFile = file(tableFileName,'w')
    tableFile.write(date+' : '+activity+'\n')
    tableFile.write('\n')
    for line in table:
        tableFile.write(line+'\n')
    tableFile.close()

    # SHOW THE SORTED LIST IN THE LOG

    log_info(date+' : '+activity)
    for line in table:
        log_info(line)
    log_info('')

#-----------------------------------------------------------

def do_rsync_backup(job):

    write_status('backup',0,job['index'])

    # if this is our first time, create a host/volume directory
    mkdir_p(topdir+'/'+job['host']+'/'+job['volume'])

    args = [
        '-al',
        '-E',
        '--delete',
        '--delete-excluded',
        '--numeric-ids',
        '--one-file-system',
#       '-v',
#       '--stats',
        '--link-dest='+topdir+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.1',
    ]
    userArgs = job['rsyncOpts'].split(' ')
    src = job['src']
    dest = topdir+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.0'

    # optional - "excludes" file
    excludes = topdir+'/'+job['host']+'/'+job['volume']+'/excludes'
    log_debug('testing for ['+excludes+']')
    if os.path.isfile(excludes):
        log_debug('"excludes" file found, adding argument')
        args.append('--exclude-from='+excludes)

    cmd = ['/usr/bin/rsync'] + args + [src, dest] # + userArgs
    log_info('running >> '+(','.join(cmd)))
    (rc,stdout,stderr) = shell_capture(cmd)

    #   0      Success
    #   1      Syntax or usage error
    #   2      Protocol incompatibility
    #   3      Errors selecting input/output files, dirs
    #   4      Requested action not supported
    #   5      Error starting client-server protocol
    #   6      Daemon unable to append to log-file
    #   10     Error in socket I/O
    #   11     Error in file I/O
    #   12     Error in rsync protocol data stream
    #   13     Errors with program diagnostics
    #   14     Error in IPC code
    #   20     Received SIGUSR1 or SIGINT
    #   21     Some error returned by waitpid()
    #   22     Error allocating core memory buffers
    #   23     Partial transfer due to error
    #   24     Partial transfer due to vanished source files
    #   25     The --max-delete limit stopped deletions
    #   30     Timeout in data send/receive
    #   35     Timeout waiting for daemon connection
    complete = True if rc in (0, 24) else False
    log_debug('rc = %d'%rc + ', %s'%('OK' if complete else 'BAD') )

    if complete:
        prefix = topdir+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'
        if os.path.isdir(prefix+'0'):
            # "touch" the timestamp
            os.utime(prefix+'0',None)
        else:
            log_debug(prefix+'0 directory was not found, marking incomplete')
            complete = False

    log_debug('backup of '+job['host']+'/'+job['volume']+' is %s' %
        ('complete' if complete else 'incomplete') )

    return complete

#-----------------------------------------------------------

def do_cp_backup(job):

    write_status('backup',0,job['index'])

    # if this is our first time, create a host/volume directory
    mkdir_p(topdir+'/'+job['host']+'/'+job['volume'])

    args = ['-alf']
    src = job['src']
    dest = topdir+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.0'

    cmd = ['/bin/cp'] + args + [src, dest]
    log_info('running >> '+(','.join(cmd)))
    (rc,stdout,stderr) = shell_capture(cmd)
    #   0      Success
    #   1      Failure
    complete = True if rc ==0 else False
    log_debug('rc = %d'%rc + ', %s'%('OK' if complete else 'BAD') )

    if complete:
        prefix = topdir+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'
        if os.path.isdir(prefix+'0'):
            # "touch" the timestamp
            os.utime(prefix+'0',None)
        else:
            log_debug(prefix+'0 directory was not found, marking incomplete')
            complete = False

    log_debug('backup of '+job['host']+'/'+job['volume']+' is %s' %
        ('complete' if complete else 'incomplete') )

    return complete

#-----------------------------------------------------------

def rotate(job):

    write_status('rotate',0,job['index'])
    prefix = topdir+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'

    # rotate the numbered backups
    if os.path.isdir(prefix+str(job['keepCount'])):
        log_debug('removing '+str(job['keepCount']))
        shutil.rmtree(prefix+str(job['keepCount']))
    rotates=[]
    for i in range(job['keepCount'],0,-1):
        if os.path.isdir(prefix+str(i-1)):
            os.rename(prefix+str(i-1),prefix+str(i))
            rotates.append(str(i-1)+'>>'+str(i))
    log_debug('rotating '+('  '.join(rotates)))

#-----------------------------------------------------------

def discard(job):

    prefix = topdir+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'
    if os.path.isdir(prefix+'0'):
        write_status('discard',0,job['index'])
        log_debug('removing 0')
        shutil.rmtree(prefix+'0')

#-----------------------------------------------------------

def do_single_pass():

    write_status('thinking',0,None)
    jobs = buildJobTable()
    jobs = updateAgesAndSort(jobs)
    tableToLogAndDisk('thinking',jobs)

    # GO THROUGH THE LIST IN ORDER, BACKING UP EACH ONE IF NEEDED

    log_info('start of single pass')
    for job in jobs:
        if job['ageSec'] < job['cycleSec']: continue
        if job['disabled']: continue
        tableToLogAndDisk("backing up '"+job['index']+"'", jobs)
        startTime = datetime.datetime.now()
        if job['tool'] == 'rsync':
            if do_rsync_backup(job):
                job['status'] = status.SUCCEEDED
                rotate(job)
            else: # not complete
                job['status'] = status.FAILED
                discard(job)
        elif job['tool'] == 'cp':
            if do_cp_backup(job):
                job['status'] = status.SUCCEEDED
                rotate(job)
            else: # not complete
                job['status'] = status.FAILED
                discard(job)
        endTime = datetime.datetime.now()
        job['lastBackupDurationSec'] = (endTime - startTime).seconds
        log_debug('done')

    log_info('end of single pass')

    # sleep for a little bit
    for i in range(sleepMin,0,-1):
        write_status('idle',i,None)
        jobs = updateAgesAndSort(jobs)
        tableToLogAndDisk('sleeping '+str(i)+' min',jobs)
        time.sleep(60)

#-----------------------------------------------------------

# START
def main():

    # FIRST -- PARSE COMMAND LINE
    usage = "usage: %prog [options]"
    parser = OptionParser(usage)
    parser.add_option("-d", "--debug", action="store_true", dest="debug")
    parser.add_option("-q", "--quiet", action="store_true", dest="quiet")
    global options
    (options, args) = parser.parse_args()
    if len(args) != 0:
        parser.error("incorrect number of arguments")

    # SET UP SERVICES

    log_init()

    # LOOK FOR PID FILE, EXIT IF FOUND

    pidfile='/var/run/'+programName+'.pid'
    try:
        with open(pidfile) as f:
            log_info("pidfile '%s' found... better look for a running process" % pidfile)
            pid = int(f.readline())
            # Check For the existence of a unix pid, send signal 0 to it.
            try:
                os.kill(pid, 0)
            except OSError:
                log_info("pid %d not found, continuing" % pid)
            else:
                log_info("pid %d is still running, exiting" % pid)
                sys.exit()
    except IOError as e:
        pass
    file(pidfile,'w').write(str(os.getpid())+'\n')

    # SET UP SUPPORT/STATUS DIRECTORY

    mkdir_p(libdir)

    # LOOP FOREVER, WORK AND SLEEP

    while True:
        do_single_pass()

    # CLEAN UP

    log_info('cleaning up')
    os.unlink(pidfile)

#-----------------------------------------------------------

if __name__ == "__main__":
     main()


